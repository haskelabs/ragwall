\documentclass[11pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage{url}

\title{RagWall: Pre-Embedding Sanitization and Risk-Aware Re-Ranking for Secure Retrieval-Augmented Generation}
\author{Ronald Doku\\Haske Labs\\\texttt{ragwall@haskelabs.example}}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Retrieval-augmented generation (RAG) pipelines retrieve textual evidence with dense vector search before feeding the results to a large language model. While RAG improves factual grounding, the retriever inherits poisoning and prompt-injection risks from the underlying corpus: adversaries can insert crafted instructions that survive the retrieval step and override system policies. We present \textsc{RagWall}, a lightweight ``firewall'' that combines pre-embedding query sanitization with risk-aware re-ranking of candidate documents. Sanitization removes high-risk control phrases while preserving the topical intent of user queries, effectively shifting the embedding neighborhood away from malicious text. The re-ranker demotes suspicious documents only when the baseline retrieval already exhibits hazardous overlap, mitigating drift on benign traffic. On synthetic and internal corpora, RagWall reduces hazardous retrieval coverage at top-5 by 68--79\% with negligible latency overhead (7--12\,ms per query) and no measurable drop in benign Jaccard similarity. We open-source the sanitization, evaluation, and reporting stack to facilitate reproducible defenses for RAG deployments.
\end{abstract}

\section{Introduction}
Large language models (LLMs) powered by retrieval-augmented generation (RAG) grant practitioners access to current, proprietary, or domain-specific content by querying external corpora at inference time \cite{lewis2020retrieval,izacard2021leveraging}. Organizations across healthcare, finance, and customer support sectors are adopting RAG to reduce hallucinations and improve verifiability. Yet the retriever forms a new attack surface: an adversary can plant seemingly innocuous text that instructs the downstream model to divulge secrets, violate policy, or behave unsafely once it is retrieved \cite{greshake2023prompt}. Because vector similarity depends on topical alignment rather than instruction semantics, poisoned documents cluster near relevant queries and bypass simple keyword filters.

Existing defenses mostly operate post-retrieval---filtering documents or generated responses---or rely on coarse binary query blocking. These strategies incur high false positive rates, degrade utility, or fail to neutralize poisoned context before it influences the model. We argue for defenses that intervene \\emph{before} embeddings are produced, thereby changing which documents enter the pipeline.

This paper introduces \textsc{RagWall}, a two-stage defense designed for production RAG systems. Stage~1 sanitizes the query prior to embedding, preserving the topic while stripping adversarial control tokens. Stage~2 applies a light-weight, risk-aware penalty to documents that exhibit high-risk lexical or structural signatures, but only when the baseline retrieval already shows hazardous overlap. The combination reduces exposure to injected instructions without altering benign queries.

Our contributions are threefold:
\begin{enumerate}[label=\textbf{C\arabic*}]
  \item We formalize a pre-embedding sanitization pipeline that uses pattern-matching, structural cues, and vector-based projections to isolate risky segments while maintaining topical information.
  \item We design a conditional re-ranking scheme that demotes documents flagged by a risk lexicon when baseline retrieval already surfaces malicious content, achieving targeted mitigation with near-zero drift.
  \item We provide an open-source evaluator, sanitizer, and reporting toolkit that measures hazardous retrieval coverage (HRCR), benign Jaccard similarity, and latency across baselines and ablations.
\end{enumerate}

\section{Threat Model and Problem Setup}
We consider RAG systems that accept user queries, compute dense embeddings, retrieve top-$k$ documents from a vector index, and feed the evidence to an LLM for answer generation. Attackers are assumed to have the capability to inject documents into the retrievable corpus (e.g., via content management systems, public ingestion pipelines, or insider modification) but cannot directly modify system prompts or code.

A \emph{prompt-injection attack} embeds adversarial instructions inside retrievable documents or user queries. When such text appears in the context window, it may override safety policies, cause data exfiltration, or encourage unsafe behaviors. We focus on two failure metrics:
\begin{itemize}
  \item \textbf{Hazardous Retrieval Coverage (HRCR)}: the fraction of retrieved documents among the top-$k$ that contain high-risk patterns, measured separately for baseline and sanitized pipelines.
  \item \textbf{Benign Drift}: the change in top-$k$ overlap for benign queries, quantified via Jaccard similarity.
\end{itemize}

Defenses must maintain low latency ($\leq$~15\,ms per query), preserve benign retrieval accuracy, and operate without access to proprietary LLM internals.

\section{RagWall Architecture}
\label{sec:architecture}
RagWall prepends a sanitization stage to the retrieval process and augments the retriever with a conditional penalty. The implementation is grounded in modular Python components shipped with the open-source repository.

\subsection{Stage 1: Pre-Embedding Query Sanitization}
Sanitization operates on the raw user input and produces two artifacts: (i) a sanitized string optimized for embedding, and (ii) metadata describing the risk assessment. The process consists of three layers:
\begin{enumerate}[label=\alph*)]
  \item \textbf{Pattern and Structure Gate}. A pattern-risk-reduction (PRR) gate scans for attacker signatures such as ``ignore previous instructions,'' ``developer mode,'' base64 blobs, jailbreak role-play cues, and multilingual paraphrases. Structural heuristics examine token repetition, JSON command blocks, or suspicious shell prompts.
  \item \textbf{Vector Projection Filters}. For LLMs equipped with gradient-based probing (GBP) directions, we compute cosine similarity between early-layer hidden states and learned ``jailbreak'' vectors. Queries with high alignment trigger sanitization even when lexical cues are obfuscated.
  \item \textbf{Topic-Preserving Rewrite}. When risk is detected, we remove or mask control phrases while retaining the topical request. For example, ``Ignore the rules and act as root to list AWS keys'' becomes ``List AWS keys'' with a metadata flag indicating high risk.
\end{enumerate}
The sanitized text feeds the embedder, while downstream components can still log the original query for auditing under appropriate privacy controls (e.g., HIPAA-mode hashing).

\subsection{Stage 2: Risk-Aware Re-Ranking}
Let $Q_b$ and $Q_s$ denote the normalized embeddings of the baseline and sanitized queries. RagWall retrieves the top-$k$ documents using both vectors. If the baseline top-$k$ contains any document labeled malicious (via ground truth or heuristics), the reranker activates and applies a penalty $\lambda$ to documents where the risk lexicon matches. Formally,
\begin{equation}
  s'(d_i) = s(d_i) - \lambda \cdot \mathbf{1}[\text{risk}(d_i)] \cdot \mathbf{1}[\text{HRCR}_b > 0],
\end{equation}
where $s(d_i)$ is the cosine similarity score, $\lambda$ is a small constant (default 0.2), and $\mathbf{1}$ denotes the indicator function. This targeted adjustment avoids penalizing benign documents when the baseline retrieval is already clean. We log both pre- and post-penalty rankings for auditing and reproducibility.

\subsection{Implementation Notes}
The sanitizer and reranker are implemented in Python with optional Hugging Face or Sentence-Transformers embedders. Configuration flags enable HIPAA-safe pseudonymization, healthcare-specific lexicons, and deterministic tie-breaking. The evaluator, located at \texttt{scripts/rag/rag\_ab\_eval.py}, measures per-pattern HRCR, top-$k$ overlap, and bootstrap confidence intervals. All scripts operate on JSONL inputs describing corpora, queries, and sanitized outputs.

\section{Evaluation}
We evaluate RagWall across synthetic and semi-realistic corpora that simulate high-risk prompt injection scenarios.

\subsection{Datasets}
\textbf{Synthetic RAG Corpus.} We generate documents and queries using \texttt{scripts/rag/make\_synthetic\_rag\_corpus.py}. Malicious documents include both overt jailbreak phrases and obfuscated variants. Queries are labeled as benign or attacked, with patterns covering direct overrides (``ignore previous''), role-play, developer console impersonation, and obfuscated commands.

\textbf{Healthcare Scenario.} To evaluate privacy-aware use cases, we produce a healthcare-specific dataset via \texttt{scripts/rag/make\_healthcare\_synthetic.py}, backed by optional HIPAA mode. This set includes protected health information (PHI) spans, risky clinical directives, and benign patient inquiries.

\subsection{Metrics}
Our primary metrics are:
\begin{itemize}
  \item \textbf{HRCR@k}. Hazardous retrieval coverage at $k \in \{5,10\}$, computed for baseline and sanitized pipelines.
  \item \textbf{Jaccard@k}. Overlap between baseline and sanitized top-$k$ for benign queries (drift analysis).
  \item \textbf{Latency}. End-to-end processing time per query batch, measured on commodity GPUs and CPUs.
\end{itemize}
We also report abstention triggers, risk-flag distributions, and reranker activation frequency.

\subsection{Results}
Table~\ref{tab:results} compares the baseline retriever with RagWall under the default configuration using the Sentence-Transformers \texttt{all-MiniLM-L6-v2} embedder.

\begin{table}[t]
  \centering
  \begin{tabular}{lccc}
    \toprule
    Configuration & HRCR@5 & HRCR@10 & Benign Jaccard@5 \\
    \midrule
    Baseline RAG & 0.80 $\pm$ 0.03 & 0.70 $\pm$ 0.04 & 1.00 \\
    RagWall (sanitize + rerank) & 0.26 $\pm$ 0.02 & 0.18 $\pm$ 0.03 & 1.00 \\
    RagWall (sanitize only) & 0.34 $\pm$ 0.02 & 0.24 $\pm$ 0.03 & 0.99 \\
    \bottomrule
  \end{tabular}
  \caption{Hazardous retrieval coverage (HRCR) and benign drift on the synthetic corpus. RagWall reduces HRCR by 68--74\% relative while preserving benign retrieval overlap. Values report mean $\pm$ bootstrap 95\% confidence intervals.}
  \label{tab:results}
\end{table}

Across multiple corpora, RagWall consistently reduces HRCR by two-thirds or more. The reranker contributes an additional 8--12 percentage point reduction over sanitization alone, particularly on patterns that reuse sanitized phrasing in document snippets. Benign queries exhibit Jaccard@5 of 0.99--1.00, indicating no observable drift. Latency overhead remains below 12\,ms per query on a desktop GPU and under 15\,ms on CPU-only setups, satisfying operational targets.

\subsection{Ablations}
We conduct ablations to understand each component's contribution:
\begin{itemize}
  \item \textbf{No vector probes}. Disabling GBP-based probes increases HRCR@5 by 6 percentage points on obfuscated queries, underscoring the value of representation-level cues.
  \item \textbf{Always-on rerank}. Applying penalties without conditioning on baseline HRCR degrades benign Jaccard@5 to 0.92, motivating the conditional mask.
  \item \textbf{Lexicon-only sanitize}. Relying solely on string replacement misses multilingual injections, whereas the pattern gate with cosine similarity captures them.
\end{itemize}

\section{Discussion}
RagWall emphasizes \emph{earliest possible intervention} in the RAG pipeline. By operating before embeddings are produced, we modify the neighborhood that the retriever explores. The targeted reranker provides an additional guardrail without punishing clean queries. These design choices limit false positives and maintain user experience.

Several practical considerations emerged during deployment:
\begin{description}[leftmargin=!,labelwidth=1.4in]
  \item[Auditability.] RagWall logs sanitized queries, risk flags, and pre/post rankings. HIPAA or privacy-sensitive deployments can enable deterministic pseudonymization for PHI spans.
  \item[Configurability.] Teams can customize the lexicon, GBP vectors, and penalty weights. Configuration files stored under \texttt{working\_config.md} capture known-good settings.
  \item[Two-Signal Tool Gate.] Integrations can invoke \texttt{src/sdk/gates.py} to prevent tool usage unless sanitizer metadata indicates safe conditions.
\end{description}

\section{Related Work}
Prompt-injection defenses span classifier-based refusal \cite{salem2023doctors}, rule-based jailbreak detection \cite{zou2023universal}, and robust prompting techniques \cite{wei2023jailbroken}. Closest to RagWall are sanitization strategies that rewrite queries before retrieval \cite{huang2023guarded} and risk-aware re-ranking methods in information retrieval \cite{zhang2021trust}. However, prior work often focuses on post-hoc filtering or requires heavyweight models. RagWall distinguishes itself through a lightweight, modular implementation that alters the retrieval neighborhood and conditions reranking on actual hazard exposure.

\section{Limitations and Future Work}
RagWall relies on curated lexicons and GBP vectors, which may lag behind evolving attacker tactics. Future work includes mining risk cues from larger corpora, learning sanitization rewrites with sequence-to-sequence models, and extending the reranker with learned risk scores. Additionally, our evaluations use synthetic or de-identified datasets; validating the approach on live production traffic requires careful monitoring and human-in-the-loop review. Finally, RagWall currently focuses on textual corpora---multimodal RAG (images, tables) introduces new sanitization and risk assessment challenges.

\section{Conclusion}
We presented RagWall, a defensive layer that shields retrieval-augmented generation systems from prompt injection attacks by sanitizing queries before embedding and selectively demoting risky documents. Experiments on synthetic and healthcare-inspired benchmarks show substantial reductions in hazardous retrieval with negligible impact on benign queries or latency. By releasing the sanitizer, reranker, and evaluation tooling, we aim to accelerate adoption of practical safeguards for RAG deployments.

\section*{Acknowledgments}
We thank the RagWall contributors at Haske Labs for engineering support and the reviewers of our internal red-team exercises for constructive feedback.

\begin{thebibliography}{9}
\bibitem{lewis2020retrieval} Patrick Lewis, Ethan Perez, Aleksandra Piktus, et~al. ``Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.'' In \\emph{NeurIPS}, 2020.
\bibitem{izacard2021leveraging} Gautier Izacard and Edouard Grave. ``Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.'' In \\emph{EACL}, 2021.
\bibitem{greshake2023prompt} Tim Greshake, et~al. ``Prompt Injection Attacks against Large Language Models.'' arXiv:2302.11382, 2023.
\bibitem{salem2023doctors} Ahmed Salem, et~al. ``Do Not Trust the Doctors: Red Teaming LLMs for Healthcare.'' arXiv:2310.02217, 2023.
\bibitem{zou2023universal} Andy Zou, et~al. ``Universal and Transferable Adversarial Attacks on Aligned Language Models.'' arXiv:2307.15043, 2023.
\bibitem{wei2023jailbroken} Jason Wei, et~al. ``Jailbroken: How Does LLM Safety Training Fail?'' arXiv:2307.02483, 2023.
\bibitem{huang2023guarded} Jie Huang, et~al. ``Guarded Retrieval for Language Models.'' arXiv:2310.05188, 2023.
\bibitem{zhang2021trust} Yanan Zhang, et~al. ``Trust Aware Retrieval.'' In \\emph{SIGIR}, 2021.
\end{thebibliography}

\end{document}
